{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "- we have seen the definitions of Transformer and Estimators with some examples from scikit-learn\n",
    "- we have built our own Transformers. \n",
    "\n",
    "We even built a class that combines two transformer in a sequence. In this chapter we will see a way to do this more efficiently through **scikit-learn pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition (Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word *pipeline* can refer to different things in data science/engineering. In our context we use this word to refer to scikit-learn pipelines:\n",
    "Theses are sequences of **transformers** that end with a final **estimator**:\n",
    "\n",
    "- the intermidiate steps of the pipelines must have fit and transform methods\n",
    "- the final step of the pipeline must have a fit method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways you can build a pipeline in scikit-learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Pipeline` is a scikit-learn class that takes a list of *steps* as inputs: the steps are tuples of (name, transformer). \n",
    "2. `make_pipeline` is a python function that represents a shorthard for ```Pipeline``` constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "Pipeline(steps = [('transformer_one', Transformer1(...)),\n",
    "                 ('transformer_two', Transformer2(...)),\n",
    "                 ....\n",
    "                 ('final_estimator', Model(...))])\n",
    "```\n",
    "Example:\n",
    "```python\n",
    "Pipeline([('vec', CountVectorizer()), ('clf', LogisticRegression())])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from scikit.pipeline import make_pipeline\n",
    "\n",
    "make_pipeline(Transformer1(...),Transformer2(...), ... , Model())\n",
    "```\n",
    "Example:\n",
    "```python\n",
    "make_pipeline(CountVectorizer(), LogisticRegression()) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the only difference between ```Pipeline``` and ```make_pipeline``` is that with Pipeline you need to specify the names of each step. This can be useful if you need to use model selection utilities (like ```GridSearch```):\n",
    "\n",
    "Example 1:\n",
    "\n",
    "```python\n",
    "pipe = Pipeline([('vec', CountVectorizer()), ('clf', LogisticRegression())])\n",
    "param_grid = [{'clf__C': [1, 10, 100, 1000]}]\n",
    "gs = GridSearchCV(pipe, param_grid)\n",
    "gs.fit(X,y)\n",
    "```\n",
    "\n",
    "Example 2:\n",
    "\n",
    "```python\n",
    "pipe = make_pipeline(CountVectorizer(), LogisticRegression())\n",
    "param_grid = [{'logisticregression__C': [1, 10, 100, 1000]}]\n",
    "gs = GridSearchCV(pipe, param_grid)\n",
    "gs.fit(X,y)\n",
    "```\n",
    "\n",
    "As a consequence of this, using Pipeline we could for example replace the final estimator (from LosticRegression to RandomForest) and the name of the estimator would stay the same. On the other hand, with make_pipeline the names of the steps are autogenerated.\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember** we talked about object oriented programming? scikit-learn Pipelines are also classes with methods:\n",
    "`fit`, `fit_transform`, `fit_predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check scikit-learn documentation for more information on Pipeline's methods: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Union and make_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain situations you want to apply a list of transformers in parallel instead of one after the other. Example: you have text data and you want to extract words frequencies using ```CountVectorizer``` and you also want to use the length of the text as feature. In this case you need to make a *union* of transformers\n",
    "\n",
    "1. ```FatureUnion``` is a class that takes a list of transformers as input, in particular a list of tuples (name, transformer) and concatenate them\n",
    "2. ```make_union``` is a function that represents a shorthand for `FeatureUnion`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from scikit.pipeline import FeatureUnion\n",
    "featunion = FeatureUnion([('count_vect', CountVectorizer(...)),('length', CustomTransformer(...))])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# you can then combine the FeatureUnion into a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('feats', featunion),\n",
    "    ('clf', Classifier())  # classifier\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/diagram_data_union.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Examples of Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word *pipeline* in Data Science can refer to many different things but in general refers to the whole processo of working with data from getting raw data to delivering something meaningful.\n",
    "\n",
    "Check here for pipelines in spark: https://spark.apache.org/docs/2.2.0/ml-pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
