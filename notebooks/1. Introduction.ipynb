{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably have heard already that **80%** of Data Scientist's work consists on finding, preparing, cleaning data and only the remaining **20%** is dedicated to modeling and analysing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/datascienceprocess.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all these workflows, it is very important to keep your code clean and organized. Moreover there are standard procedures (ex: replacing missing values) that need to be consistent and automated so that when you have new data coming, exactly the same transformation is applied.\n",
    "**Pipelines** are a way to overcome these problems and keep your code clean!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, the data you obtain is very heterogenous: it is not in a matrix form already prepared and it contains a mixture of data type: categorical features, numerical features, text. Often you also have the problem of missing values.\n",
    "\n",
    "**Pipelines** can be seen as a series of transformations that you can apply to your data one after the other and that can get your data to a format that can be \"readable\" by a model. You can even add the final step (train your model + make predictions) to the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/diagram_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data = read_file('..')\n",
    "\n",
    "data_num = extract_num(data)\n",
    "data_cat = extract_cat(data)\n",
    "data_bool = extract_bool(data)\n",
    "\n",
    "data_num = replace_missing(data_num)\n",
    "data_cat = replace_missing(data_cat)\n",
    "data_cat = get_num_features(data_cat)\n",
    "\n",
    "concatenate...\n",
    "\n",
    "split_train_test...\n",
    "\n",
    "model fit....\n",
    "\n",
    "model predict\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to make some changes, add some other transformations, normalize, etc. With the code above you need to keep track at every step and you might end up with a mess of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"pipeline\" in data science might refer to different concepts. **Here** we will consider pipelines in scikit-learn \n",
    "- we will use python\n",
    "- we will work with scikit-learn pipelines (https://scikit-learn.org/stable/modules/compose.html)\n",
    "- we will use pandas dataframe as inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Object oriented programming: Classes in Python\n",
    "2. Special type of classes in Python: Transformers and Estimators\n",
    "3. Pipelines\n",
    "4. Feature Union\n",
    "\n",
    "Throught the course we will have a lot of exercises and hands on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
