{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/francesca/Desktop/FrancescaDSRPipelines\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_table('./sentiment_data/processed_stars/books/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./sentiment_data/Musical_Instruments_5.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>5</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>Jake</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>02 21, 2014</td>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  1384719342    [0, 0]        5   \n",
       "1  1384719342  [13, 14]        5   \n",
       "2  1384719342    [1, 1]        5   \n",
       "3  1384719342    [0, 0]        5   \n",
       "4  1384719342    [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Not much to write about here, but it does exac...  02 28, 2014   \n",
       "1  The product does exactly as it should and is q...  03 16, 2013   \n",
       "2  The primary job of this device is to block the...  08 28, 2013   \n",
       "3  Nice windscreen protects my MXL mic and preven...  02 14, 2014   \n",
       "4  This pop filter is great. It looks and perform...  02 21, 2014   \n",
       "\n",
       "       reviewerID                                      reviewerName  \\\n",
       "0  A2IBPI20UZIR0U  cassandra tu \"Yeah, well, that's just like, u...   \n",
       "1  A14VAT5EAX3D9S                                              Jake   \n",
       "2  A195EZSQDW3E21                     Rick Bennette \"Rick Bennette\"   \n",
       "3  A2C00NNG1ZQQG2                         RustyBill \"Sunday Rocker\"   \n",
       "4   A94QU4C90B1AX                                     SEAN MASLANKA   \n",
       "\n",
       "                                 summary  unixReviewTime  \n",
       "0                                   good      1393545600  \n",
       "1                                   Jake      1363392000  \n",
       "2                   It Does The Job Well      1377648000  \n",
       "3          GOOD WINDSCREEN FOR THE MONEY      1392336000  \n",
       "4  No more pops when I record my vocals.      1392940800  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10261"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lim = df[(df.overall==5) | (df.overall==1)].copy()\n",
    "df_lim.loc[df.overall==5,'overall']=1\n",
    "df_lim.loc[df.overall==1,'overall']=0\n",
    "#df_lim.loc[df.overall==2,'overall']=0\n",
    "#df_lim.loc[df.overall==3,'overall']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6938"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_lim[df_lim.overall==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1239"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_lim[df_lim.overall==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider first only reviewText and overall (=rating) and let's try to make a prediction of the rating just based on the reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df[['reviewText','overall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reviews by rating there are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "df_t.groupby('overall').count().reset_index()\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  reviewText\n",
       "0        1         217\n",
       "1        2         250\n",
       "2        3         772\n",
       "3        4        2084\n",
       "4        5        6938"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.groupby('overall').count().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a couple of examples of bad reviews (overall=1 or 2) and of good reviews (overall=5 or 4) to have a feeling how users write..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I now use this cable to run from the output of my pedal chain to the input of my Fender Amp. After I bought Monster Cable to hook up my pedal board I thought I would try another one and update my guitar. I had been using a high end Planet Waves cable that I bought in the 1980's... Once I found out the input jacks on the new Monster cable didn't fit into the Fender Strat jack I was a little disappointed... I didn't return it and as stated I use it for the output on the pedal board. Save your money... I went back to my Planet Waves Cable...I payed $30.00 back in the eighties for the Planet Waves which now comes in at around $50.00. What I'm getting at is you get what you pay for. I thought Waves was a lot of money back in the day...but I haven't bought a guitar cable since this one...20 plus years and still working...Planet Waves wins.\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t[df_t.overall==3].iloc[0]['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.reviewText.values\n",
    "y = df.overall.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, \n",
    "                                          y,\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10261"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9234,)"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 198)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.where(y_tr==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "--------------------\n",
    "\n",
    "Different types of vectorizers:\n",
    "\n",
    "<ul>\n",
    "<li>```sklearn.feature_extraction.text.CountVectorizer``` - Counts the number of times a word appears in the text</li>\n",
    "<li>```sklearn.feature_extraction.text.TfidfVectorizer``` - Weighs the words according to the importance of the word in the context of whole collection. Is the word ```the``` important if it appears in all documents?</li>\n",
    "<li>```sklearn.feature_extraction.text.HashingVectorizer``` - Useful when you don't know the vocabulary upfront. Feature number is calculated as ```hash(token) % vocabulary_size```.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "-------------------\n",
    "1. Use ```CountVectorizer``` / ```TfidfVectorizer``` to fit the collection of documents\n",
    "2. How many unique tokens are there in text? Print some examples (ie first few hundred).\n",
    "3. What methods you can use to reduce this number? \n",
    "   - Check out and experiment with the arguments: ```ngram_range```, ```min_df```. How the vocabulary size changes with each change?\n",
    "   - Would you make use of stop_words? Check ```CountVectorizer``` documentation\n",
    "   - What would you replace / delete from the text?\n",
    "4. Write a custom function `clean_text` that accepts a text as input and transforms it (remove/hash numbers, delete short/long words etc.)\n",
    "5. (Extra points) When would you use ```HashingVectorizer```?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "\n",
    "vectorizers = [\n",
    "     (\"simple\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "    ,(\"preprocessing + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "    print('and' in vect.get_feature_names())\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple\n",
      "['00', '000', '000s', '001', '007', '008', '008ex', '009', '009s', '00e']\n",
      "15961\n",
      "preprocessing\n",
      "['aa', 'aaa', 'abalone', 'abandon', 'abbe', 'abehringer', 'abelton', 'abercrombie', 'ability', 'abitity']\n",
      "14928\n",
      "preprocessing + min_df=10\n",
      "['ability', 'able', 'ableton', 'about', 'above', 'absolute', 'absolutely', 'abuse', 'ac', 'acceptable']\n",
      "3002\n",
      "preprocessing + min_df=10 + stop_words\n",
      "['ability', 'able', 'ableton', 'absolute', 'absolutely', 'abuse', 'ac', 'acceptable', 'access', 'accessible']\n",
      "2752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "\n",
    "vectorizers = [\n",
    "     (\"simple\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "    ,(\"preprocessing + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "#    print('and' in vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization\n",
    "------------------\n",
    "\n",
    "Stemming and Lemmatization are two linguistic normalization techniques that consists on grouping together words that derivate from the same origing and have the same meaning:\n",
    "\n",
    "    connection\n",
    "    connections\n",
    "    connective     --->   connect\n",
    "    connected\n",
    "    connecting\n",
    "\n",
    "**Stemming** works by reducing a group of words to the same *stem* based on the origin of the words in the group. The stem does not necessarly have to be a word in the language. With **lemmatization**, on the other hand, makes sure that the root word is a proper word (*lemma*) in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/francesca/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/francesca/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('universally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'universally'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('universally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'universally'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer.lemmatize('universally',pos=wordnet.ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('is',pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "Extend the function `clean_text` created above to a function that, after cleaning the text, applies a stemming technique to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_stem(text):\n",
    "    t = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(t)    # alternative: [w for w in t.split()]\n",
    "    stemmed_tokens = [stemmer.stem(w) for w in tokens] \n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    return stemmed_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_text_stem(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple\n",
      "['00', '000', '000s', '001', '007', '008', '008ex', '009', '009s', '00e']\n",
      "15961\n",
      "preprocessing\n",
      "['aa', 'aaa', 'abalone', 'abandon', 'abbe', 'abehringer', 'abelton', 'abercrombie', 'ability', 'abitity']\n",
      "14928\n",
      "preprocessing + min_df=10\n",
      "['ability', 'able', 'ableton', 'about', 'above', 'absolute', 'absolutely', 'abuse', 'ac', 'acceptable']\n",
      "3002\n",
      "preprocessing + min_df=10 + stop_words\n",
      "['ability', 'able', 'ableton', 'absolute', 'absolutely', 'abuse', 'ac', 'acceptable', 'access', 'accessible']\n",
      "2752\n",
      "stemming + min_df=10 + stop_words\n",
      "['abil', 'abl', 'ableton', 'abov', 'absolut', 'abus', 'ac', 'accept', 'access', 'accessori']\n",
      "2187\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [\n",
    "     (\"simple\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "    ,(\"preprocessing + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "    ,(\"stemming + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text_stem,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "]\n",
    "\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "#    print('and' in vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it into a pipeline\n",
    "----------------------\n",
    "\n",
    "Now that we know how to transform text data, let's put it into a pipeline.\n",
    "1. Create a pipeline with `CountVectorizer`, `StandardScaler` and `SGDClassifier` as your final algorithm\n",
    "    a) use alternative format for pipeline definition when you name the steps - refer to the documentation how to do this\n",
    "2. Using ```sklearn.metrics.classification_report``` create a report about your classifier\n",
    "3. Play with text preprocessing in ```CountVectorizer``` to see if the model improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f13c8f395d0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/francesca/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/france.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f13c8f395d0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/francesca/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/france.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         self.io_loop = ioloop.IOLoop.current()\n    477         try:\n--> 478             self.io_loop.start()\n        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    479         except KeyboardInterrupt:\n    480             pass\n    481 \n    482 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 1, 12, 14, 53, 5, 406691, tzinfo=tzutc()), 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'session': '98ca2b8c584445ca8a43251228814096', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'98ca2b8c584445ca8a43251228814096']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 1, 12, 14, 53, 5, 406691, tzinfo=tzutc()), 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'session': '98ca2b8c584445ca8a43251228814096', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'98ca2b8c584445ca8a43251228814096'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 1, 12, 14, 53, 5, 406691, tzinfo=tzutc()), 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'session': '98ca2b8c584445ca8a43251228814096', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-548-4e3200db33dc>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f132b4747f0, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f132dcf0780, file \"<ipython-input-548-4e3200db33dc>\", line 13>\n        result = <ExecutionResult object at 7f132b4747f0, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f132dcf0780, file \"<ipython-input-548-4e3200db33dc>\", line 13>, result=<ExecutionResult object at 7f132b4747f0, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f132dcf0780, file \"<ipython-input-548-4e3200db33dc>\", line 13>\n        self.user_global_ns = {'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'CountEmojis': <class '__main__.CountEmojis'>, 'CountPunctuation': <class '__main__.CountPunctuation'>, 'CountUpper': <class '__main__.CountUpper'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'FeatureUnion': <class 'sklearn.pipeline.FeatureUnion'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import numpy as np\\nimport pandas as pd', \"df = pd.read_csv('./sentiment labelled sentences/imbd_labelled.txt')\", \"get_ipython().system('pwd')\", \"df = pd.read_('./sentiment_data/all/imbd_labelled.txt')\", \"df = pd.read_csv('./sentiment_data/all/imbd_labelled.txt')\", \"df = pd.read_csv('./sentiment_data/all/imdb_labelled.txt')\", \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt')\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt',header=True)\", \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt')\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt',header=None)\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_la...txt',header=None)\\ndf.columns = ['review','label']\", 'df.head()', 'len(df)', \"df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)\", \"#df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)\", \"df = pd.read_table('./sentiment_data/processed_stars/books/train')\", ...], 'LengthExtractor': <class '__main__.LengthExtractor'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, ...}\n        self.user_ns = {'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'CountEmojis': <class '__main__.CountEmojis'>, 'CountPunctuation': <class '__main__.CountPunctuation'>, 'CountUpper': <class '__main__.CountUpper'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'FeatureUnion': <class 'sklearn.pipeline.FeatureUnion'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import numpy as np\\nimport pandas as pd', \"df = pd.read_csv('./sentiment labelled sentences/imbd_labelled.txt')\", \"get_ipython().system('pwd')\", \"df = pd.read_('./sentiment_data/all/imbd_labelled.txt')\", \"df = pd.read_csv('./sentiment_data/all/imbd_labelled.txt')\", \"df = pd.read_csv('./sentiment_data/all/imdb_labelled.txt')\", \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt')\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt',header=True)\", \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt')\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt',header=None)\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_la...txt',header=None)\\ndf.columns = ['review','label']\", 'df.head()', 'len(df)', \"df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)\", \"#df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)\", \"df = pd.read_table('./sentiment_data/processed_stars/books/train')\", ...], 'LengthExtractor': <class '__main__.LengthExtractor'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/francesca/Desktop/FrancescaDSRPipelines/<ipython-input-548-4e3200db33dc> in <module>()\n     11                 ('clf', RandomForestClassifier())])\n     12 \n     13 preds = cross_val_predict(clf, \n     14                           X_tr, \n     15                           y_tr, \n---> 16                           cv=8, n_jobs=-1, verbose=True)\n     17 \n     18 print(classification_report(y_tr, preds))\n     19 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py in cross_val_predict(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], y=4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, cv=sklearn.cross_validation.StratifiedKFold(labels=... 5], n_folds=8, shuffle=False, random_state=None), n_jobs=-1, verbose=True, fit_params=None, pre_dispatch='2*n_jobs')\n   1379     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n   1380                         pre_dispatch=pre_dispatch)\n   1381     preds_blocks = parallel(delayed(_fit_and_predict)(clone(estimator), X, y,\n   1382                                                       train, test, verbose,\n   1383                                                       fit_params)\n-> 1384                             for train, test in cv)\n        cv = sklearn.cross_validation.StratifiedKFold(labels=... 5], n_folds=8, shuffle=False, random_state=None)\n   1385 \n   1386     preds = [p for p, _ in preds_blocks]\n   1387     locs = np.concatenate([loc for _, loc in preds_blocks])\n   1388     if not _check_is_partition(locs, _num_samples(X)):\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object cross_val_predict.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Sat Jan 12 15:53:06 2019\nPID: 28321               Python 3.6.4: /home/francesca/anaconda3/bin/python\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_predict>, (Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]),                                               re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], 4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), array([   0,    1,    2, ..., 1490, 1498, 1508]), True, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_predict>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]),                                               re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], 4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), array([   0,    1,    2, ..., 1490, 1498, 1508]), True, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py in _fit_and_predict(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], y=4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, train=array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), test=array([   0,    1,    2, ..., 1490, 1498, 1508]), verbose=True, fit_params={})\n   1444     X_test, _ = _safe_split(estimator, X, y, test, train)\n   1445 \n   1446     if y_train is None:\n   1447         estimator.fit(X_train, **fit_params)\n   1448     else:\n-> 1449         estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...one, verbose=0,\n            warm_start=False))])>\n        X_train =                                               re...st plugs for One Spot!  \n\n[8077 rows x 2 columns]\n        y_train = 8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64\n        fit_params = {}\n   1450     preds = estimator.predict(X_test)\n   1451     return preds, test\n   1452 \n   1453 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[8077 rows x 2 columns], y=8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64, **fit_params={})\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n    248         Xt, fit_params = self._fit(X, y, **fit_params)\n    249         if self._final_estimator is not None:\n--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        Xt = <2x2 sparse matrix of type '<class 'numpy.float6... stored elements in Compressed Sparse Row format>\n        y = 8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64\n        fit_params = {}\n    251         return self\n    252 \n    253     def fit_transform(self, X, y=None, **fit_params):\n    254         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 9\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    774         self.n_completed_tasks = 0\n    775         try:\n    776             # Only set self._iterating to True if at least a batch\n    777             # was dispatched. In particular this covers the edge\n    778             # case of Parallel used with an exhausted iterator.\n--> 779             while self.dispatch_one_batch(iterator):\n        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>\n        iterator = <generator object BaseForest.fit.<locals>.<genexpr>>\n    780                 self._iterating = True\n    781             else:\n    782                 self._iterating = False\n    783 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))\n    621             if len(tasks) == 0:\n    622                 # No more tasks available in the iterator: tell caller to stop.\n    623                 return False\n    624             else:\n--> 625                 self._dispatch(tasks)\n        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>\n        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    626                 return True\n    627 \n    628     def _print(self, msg, msg_args):\n    629         \"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    583         self.n_dispatched_tasks += len(batch)\n    584         self.n_dispatched_batches += 1\n    585 \n    586         dispatch_timestamp = time.time()\n    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n--> 588         job = self._backend.apply_async(batch, callback=cb)\n        job = undefined\n        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>\n        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>\n    589         self._jobs.append(job)\n    590 \n    591     def dispatch_next(self):\n    592         \"\"\"Dispatch more data for parallel processing\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)\n    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')\n    107         return 1\n    108 \n    109     def apply_async(self, func, callback=None):\n    110         \"\"\"Schedule a func to be run\"\"\"\n--> 111         result = ImmediateResult(func)\n        result = undefined\n        func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    112         if callback:\n    113             callback(result)\n    114         return result\n    115 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    327 \n    328 class ImmediateResult(object):\n    329     def __init__(self, batch):\n    330         # Don't delay the application, to avoid keeping the input\n    331         # arguments in memory\n--> 332         self.results = batch()\n        self.results = undefined\n        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    333 \n    334     def get(self):\n    335         return self.results\n    336 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), None, 0, 10), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), None, 0, 10)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=None, tree_idx=0, n_trees=10, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...       random_state=2047869876, splitter='best')>\n        X = <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>\n        y = array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=array([1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=array([1., 1.]), check_input=False, X_idx_sorted=None)\n    231 \n    232         self.max_features_ = max_features\n    233 \n    234         if len(y) != n_samples:\n    235             raise ValueError(\"Number of labels=%d does not match \"\n--> 236                              \"number of samples=%d\" % (len(y), n_samples))\n        y = array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]])\n        n_samples = 2\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n\nValueError: Number of labels=8077 does not match number of samples=2\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py\", line 1449, in _fit_and_predict\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\", line 250, in fit\n    self._final_estimator.fit(Xt, y, **fit_params)\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\", line 328, in fit\n    for i, t in enumerate(trees))\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 779, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 625, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 588, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 111, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 332, in __init__\n    self.results = batch()\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\", line 121, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\", line 790, in fit\n    X_idx_sorted=X_idx_sorted)\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\", line 236, in fit\n    \"number of samples=%d\" % (len(y), n_samples))\nValueError: Number of labels=8077 does not match number of samples=2\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/francesca/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Sat Jan 12 15:53:06 2019\nPID: 28321               Python 3.6.4: /home/francesca/anaconda3/bin/python\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_predict>, (Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]),                                               re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], 4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), array([   0,    1,    2, ..., 1490, 1498, 1508]), True, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_predict>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]),                                               re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], 4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), array([   0,    1,    2, ..., 1490, 1498, 1508]), True, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py in _fit_and_predict(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], y=4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, train=array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), test=array([   0,    1,    2, ..., 1490, 1498, 1508]), verbose=True, fit_params={})\n   1444     X_test, _ = _safe_split(estimator, X, y, test, train)\n   1445 \n   1446     if y_train is None:\n   1447         estimator.fit(X_train, **fit_params)\n   1448     else:\n-> 1449         estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...one, verbose=0,\n            warm_start=False))])>\n        X_train =                                               re...st plugs for One Spot!  \n\n[8077 rows x 2 columns]\n        y_train = 8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64\n        fit_params = {}\n   1450     preds = estimator.predict(X_test)\n   1451     return preds, test\n   1452 \n   1453 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[8077 rows x 2 columns], y=8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64, **fit_params={})\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n    248         Xt, fit_params = self._fit(X, y, **fit_params)\n    249         if self._final_estimator is not None:\n--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        Xt = <2x2 sparse matrix of type '<class 'numpy.float6... stored elements in Compressed Sparse Row format>\n        y = 8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64\n        fit_params = {}\n    251         return self\n    252 \n    253     def fit_transform(self, X, y=None, **fit_params):\n    254         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 9\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    774         self.n_completed_tasks = 0\n    775         try:\n    776             # Only set self._iterating to True if at least a batch\n    777             # was dispatched. In particular this covers the edge\n    778             # case of Parallel used with an exhausted iterator.\n--> 779             while self.dispatch_one_batch(iterator):\n        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>\n        iterator = <generator object BaseForest.fit.<locals>.<genexpr>>\n    780                 self._iterating = True\n    781             else:\n    782                 self._iterating = False\n    783 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))\n    621             if len(tasks) == 0:\n    622                 # No more tasks available in the iterator: tell caller to stop.\n    623                 return False\n    624             else:\n--> 625                 self._dispatch(tasks)\n        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>\n        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    626                 return True\n    627 \n    628     def _print(self, msg, msg_args):\n    629         \"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    583         self.n_dispatched_tasks += len(batch)\n    584         self.n_dispatched_batches += 1\n    585 \n    586         dispatch_timestamp = time.time()\n    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n--> 588         job = self._backend.apply_async(batch, callback=cb)\n        job = undefined\n        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>\n        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>\n    589         self._jobs.append(job)\n    590 \n    591     def dispatch_next(self):\n    592         \"\"\"Dispatch more data for parallel processing\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)\n    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')\n    107         return 1\n    108 \n    109     def apply_async(self, func, callback=None):\n    110         \"\"\"Schedule a func to be run\"\"\"\n--> 111         result = ImmediateResult(func)\n        result = undefined\n        func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    112         if callback:\n    113             callback(result)\n    114         return result\n    115 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    327 \n    328 class ImmediateResult(object):\n    329     def __init__(self, batch):\n    330         # Don't delay the application, to avoid keeping the input\n    331         # arguments in memory\n--> 332         self.results = batch()\n        self.results = undefined\n        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    333 \n    334     def get(self):\n    335         return self.results\n    336 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), None, 0, 10), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), None, 0, 10)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=None, tree_idx=0, n_trees=10, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...       random_state=2047869876, splitter='best')>\n        X = <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>\n        y = array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=array([1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=array([1., 1.]), check_input=False, X_idx_sorted=None)\n    231 \n    232         self.max_features_ = max_features\n    233 \n    234         if len(y) != n_samples:\n    235             raise ValueError(\"Number of labels=%d does not match \"\n--> 236                              \"number of samples=%d\" % (len(y), n_samples))\n        y = array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]])\n        n_samples = 2\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n\nValueError: Number of labels=8077 does not match number of samples=2\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Sat Jan 12 15:53:06 2019\nPID: 28321               Python 3.6.4: /home/francesca/anaconda3/bin/python\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_predict>, (Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]),                                               re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], 4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), array([   0,    1,    2, ..., 1490, 1498, 1508]), True, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_predict>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]),                                               re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], 4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), array([   0,    1,    2, ..., 1490, 1498, 1508]), True, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py in _fit_and_predict(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], y=4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, train=array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), test=array([   0,    1,    2, ..., 1490, 1498, 1508]), verbose=True, fit_params={})\n   1444     X_test, _ = _safe_split(estimator, X, y, test, train)\n   1445 \n   1446     if y_train is None:\n   1447         estimator.fit(X_train, **fit_params)\n   1448     else:\n-> 1449         estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...one, verbose=0,\n            warm_start=False))])>\n        X_train =                                               re...st plugs for One Spot!  \n\n[8077 rows x 2 columns]\n        y_train = 8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64\n        fit_params = {}\n   1450     preds = estimator.predict(X_test)\n   1451     return preds, test\n   1452 \n   1453 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[8077 rows x 2 columns], y=8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64, **fit_params={})\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n    248         Xt, fit_params = self._fit(X, y, **fit_params)\n    249         if self._final_estimator is not None:\n--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        Xt = <2x2 sparse matrix of type '<class 'numpy.float6... stored elements in Compressed Sparse Row format>\n        y = 8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64\n        fit_params = {}\n    251         return self\n    252 \n    253     def fit_transform(self, X, y=None, **fit_params):\n    254         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 9\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    774         self.n_completed_tasks = 0\n    775         try:\n    776             # Only set self._iterating to True if at least a batch\n    777             # was dispatched. In particular this covers the edge\n    778             # case of Parallel used with an exhausted iterator.\n--> 779             while self.dispatch_one_batch(iterator):\n        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>\n        iterator = <generator object BaseForest.fit.<locals>.<genexpr>>\n    780                 self._iterating = True\n    781             else:\n    782                 self._iterating = False\n    783 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))\n    621             if len(tasks) == 0:\n    622                 # No more tasks available in the iterator: tell caller to stop.\n    623                 return False\n    624             else:\n--> 625                 self._dispatch(tasks)\n        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>\n        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    626                 return True\n    627 \n    628     def _print(self, msg, msg_args):\n    629         \"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    583         self.n_dispatched_tasks += len(batch)\n    584         self.n_dispatched_batches += 1\n    585 \n    586         dispatch_timestamp = time.time()\n    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n--> 588         job = self._backend.apply_async(batch, callback=cb)\n        job = undefined\n        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>\n        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>\n    589         self._jobs.append(job)\n    590 \n    591     def dispatch_next(self):\n    592         \"\"\"Dispatch more data for parallel processing\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)\n    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')\n    107         return 1\n    108 \n    109     def apply_async(self, func, callback=None):\n    110         \"\"\"Schedule a func to be run\"\"\"\n--> 111         result = ImmediateResult(func)\n        result = undefined\n        func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    112         if callback:\n    113             callback(result)\n    114         return result\n    115 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    327 \n    328 class ImmediateResult(object):\n    329     def __init__(self, batch):\n    330         # Don't delay the application, to avoid keeping the input\n    331         # arguments in memory\n--> 332         self.results = batch()\n        self.results = undefined\n        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    333 \n    334     def get(self):\n    335         return self.results\n    336 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), None, 0, 10), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), None, 0, 10)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=None, tree_idx=0, n_trees=10, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...       random_state=2047869876, splitter='best')>\n        X = <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>\n        y = array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=array([1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=array([1., 1.]), check_input=False, X_idx_sorted=None)\n    231 \n    232         self.max_features_ = max_features\n    233 \n    234         if len(y) != n_samples:\n    235             raise ValueError(\"Number of labels=%d does not match \"\n--> 236                              \"number of samples=%d\" % (len(y), n_samples))\n        y = array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]])\n        n_samples = 2\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n\nValueError: Number of labels=8077 does not match number of samples=2\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-548-4e3200db33dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                           \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                           \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                           cv=8, n_jobs=-1, verbose=True)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                                                       \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                                                       fit_params)\n\u001b[0;32m-> 1384\u001b[0;31m                             for train, test in cv)\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds_blocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f13c8f395d0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/francesca/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/france.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f13c8f395d0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/francesca/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/france.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         self.io_loop = ioloop.IOLoop.current()\n    477         try:\n--> 478             self.io_loop.start()\n        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    479         except KeyboardInterrupt:\n    480             pass\n    481 \n    482 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 1, 12, 14, 53, 5, 406691, tzinfo=tzutc()), 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'session': '98ca2b8c584445ca8a43251228814096', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'98ca2b8c584445ca8a43251228814096']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 1, 12, 14, 53, 5, 406691, tzinfo=tzutc()), 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'session': '98ca2b8c584445ca8a43251228814096', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'98ca2b8c584445ca8a43251228814096'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 1, 12, 14, 53, 5, 406691, tzinfo=tzutc()), 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'session': '98ca2b8c584445ca8a43251228814096', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '6d4dc8551d184e8c8220b23b975c8506', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from sklearn.pipeline import Pipeline\\nfrom sklea...rue)\\n\\nprint(classification_report(y_tr, preds))\\n\\n', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-548-4e3200db33dc>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f132b4747f0, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f132dcf0780, file \"<ipython-input-548-4e3200db33dc>\", line 13>\n        result = <ExecutionResult object at 7f132b4747f0, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f132dcf0780, file \"<ipython-input-548-4e3200db33dc>\", line 13>, result=<ExecutionResult object at 7f132b4747f0, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f132dcf0780, file \"<ipython-input-548-4e3200db33dc>\", line 13>\n        self.user_global_ns = {'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'CountEmojis': <class '__main__.CountEmojis'>, 'CountPunctuation': <class '__main__.CountPunctuation'>, 'CountUpper': <class '__main__.CountUpper'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'FeatureUnion': <class 'sklearn.pipeline.FeatureUnion'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import numpy as np\\nimport pandas as pd', \"df = pd.read_csv('./sentiment labelled sentences/imbd_labelled.txt')\", \"get_ipython().system('pwd')\", \"df = pd.read_('./sentiment_data/all/imbd_labelled.txt')\", \"df = pd.read_csv('./sentiment_data/all/imbd_labelled.txt')\", \"df = pd.read_csv('./sentiment_data/all/imdb_labelled.txt')\", \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt')\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt',header=True)\", \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt')\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt',header=None)\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_la...txt',header=None)\\ndf.columns = ['review','label']\", 'df.head()', 'len(df)', \"df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)\", \"#df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)\", \"df = pd.read_table('./sentiment_data/processed_stars/books/train')\", ...], 'LengthExtractor': <class '__main__.LengthExtractor'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, ...}\n        self.user_ns = {'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'CountEmojis': <class '__main__.CountEmojis'>, 'CountPunctuation': <class '__main__.CountPunctuation'>, 'CountUpper': <class '__main__.CountUpper'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'FeatureUnion': <class 'sklearn.pipeline.FeatureUnion'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import numpy as np\\nimport pandas as pd', \"df = pd.read_csv('./sentiment labelled sentences/imbd_labelled.txt')\", \"get_ipython().system('pwd')\", \"df = pd.read_('./sentiment_data/all/imbd_labelled.txt')\", \"df = pd.read_csv('./sentiment_data/all/imbd_labelled.txt')\", \"df = pd.read_csv('./sentiment_data/all/imdb_labelled.txt')\", \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt')\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt',header=True)\", \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt')\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_labelled.txt',header=None)\", 'df.head()', \"df = pd.read_table('./sentiment_data/all/imdb_la...txt',header=None)\\ndf.columns = ['review','label']\", 'df.head()', 'len(df)', \"df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)\", \"#df = pd.read_table('./data/aclImdb/data_all/full_train.txt',header=None)\", \"df = pd.read_table('./sentiment_data/processed_stars/books/train')\", ...], 'LengthExtractor': <class '__main__.LengthExtractor'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/francesca/Desktop/FrancescaDSRPipelines/<ipython-input-548-4e3200db33dc> in <module>()\n     11                 ('clf', RandomForestClassifier())])\n     12 \n     13 preds = cross_val_predict(clf, \n     14                           X_tr, \n     15                           y_tr, \n---> 16                           cv=8, n_jobs=-1, verbose=True)\n     17 \n     18 print(classification_report(y_tr, preds))\n     19 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py in cross_val_predict(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], y=4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, cv=sklearn.cross_validation.StratifiedKFold(labels=... 5], n_folds=8, shuffle=False, random_state=None), n_jobs=-1, verbose=True, fit_params=None, pre_dispatch='2*n_jobs')\n   1379     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n   1380                         pre_dispatch=pre_dispatch)\n   1381     preds_blocks = parallel(delayed(_fit_and_predict)(clone(estimator), X, y,\n   1382                                                       train, test, verbose,\n   1383                                                       fit_params)\n-> 1384                             for train, test in cv)\n        cv = sklearn.cross_validation.StratifiedKFold(labels=... 5], n_folds=8, shuffle=False, random_state=None)\n   1385 \n   1386     preds = [p for p, _ in preds_blocks]\n   1387     locs = np.concatenate([loc for _, loc in preds_blocks])\n   1388     if not _check_is_partition(locs, _num_samples(X)):\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object cross_val_predict.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Sat Jan 12 15:53:06 2019\nPID: 28321               Python 3.6.4: /home/francesca/anaconda3/bin/python\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_predict>, (Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]),                                               re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], 4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), array([   0,    1,    2, ..., 1490, 1498, 1508]), True, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_predict>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]),                                               re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], 4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), array([   0,    1,    2, ..., 1490, 1498, 1508]), True, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py in _fit_and_predict(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[9234 rows x 2 columns], y=4306     5\n6556     5\n551      5\n4598     4\n4178...2     5\nName: overall, Length: 9234, dtype: int64, train=array([ 997, 1040, 1046, ..., 9231, 9232, 9233]), test=array([   0,    1,    2, ..., 1490, 1498, 1508]), verbose=True, fit_params={})\n   1444     X_test, _ = _safe_split(estimator, X, y, test, train)\n   1445 \n   1446     if y_train is None:\n   1447         estimator.fit(X_train, **fit_params)\n   1448     else:\n-> 1449         estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...one, verbose=0,\n            warm_start=False))])>\n        X_train =                                               re...st plugs for One Spot!  \n\n[8077 rows x 2 columns]\n        y_train = 8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64\n        fit_params = {}\n   1450     preds = estimator.predict(X_test)\n   1451     return preds, test\n   1452 \n   1453 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...None, verbose=0,\n            warm_start=False))]), X=                                              re...st plugs for One Spot!  \n\n[8077 rows x 2 columns], y=8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64, **fit_params={})\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n    248         Xt, fit_params = self._fit(X, y, **fit_params)\n    249         if self._final_estimator is not None:\n--> 250             self._final_estimator.fit(Xt, y, **fit_params)\n        self._final_estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        Xt = <2x2 sparse matrix of type '<class 'numpy.float6... stored elements in Compressed Sparse Row format>\n        y = 8453     2\n9556     2\n7077     2\n8103     2\n5608...2     5\nName: overall, Length: 8077, dtype: int64\n        fit_params = {}\n    251         return self\n    252 \n    253     def fit_transform(self, X, y=None, **fit_params):\n    254         \"\"\"Fit the model and transform with the final estimator\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 9\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    774         self.n_completed_tasks = 0\n    775         try:\n    776             # Only set self._iterating to True if at least a batch\n    777             # was dispatched. In particular this covers the edge\n    778             # case of Parallel used with an exhausted iterator.\n--> 779             while self.dispatch_one_batch(iterator):\n        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>\n        iterator = <generator object BaseForest.fit.<locals>.<genexpr>>\n    780                 self._iterating = True\n    781             else:\n    782                 self._iterating = False\n    783 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))\n    621             if len(tasks) == 0:\n    622                 # No more tasks available in the iterator: tell caller to stop.\n    623                 return False\n    624             else:\n--> 625                 self._dispatch(tasks)\n        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>\n        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    626                 return True\n    627 \n    628     def _print(self, msg, msg_args):\n    629         \"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    583         self.n_dispatched_tasks += len(batch)\n    584         self.n_dispatched_batches += 1\n    585 \n    586         dispatch_timestamp = time.time()\n    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n--> 588         job = self._backend.apply_async(batch, callback=cb)\n        job = undefined\n        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>\n        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>\n    589         self._jobs.append(job)\n    590 \n    591     def dispatch_next(self):\n    592         \"\"\"Dispatch more data for parallel processing\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)\n    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')\n    107         return 1\n    108 \n    109     def apply_async(self, func, callback=None):\n    110         \"\"\"Schedule a func to be run\"\"\"\n--> 111         result = ImmediateResult(func)\n        result = undefined\n        func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    112         if callback:\n    113             callback(result)\n    114         return result\n    115 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    327 \n    328 class ImmediateResult(object):\n    329     def __init__(self, batch):\n    330         # Don't delay the application, to avoid keeping the input\n    331         # arguments in memory\n--> 332         self.results = batch()\n        self.results = undefined\n        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>\n    333 \n    334     def get(self):\n    335         return self.results\n    336 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), None, 0, 10), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), None, 0, 10)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=None, tree_idx=0, n_trees=10, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...       random_state=2047869876, splitter='best')>\n        X = <2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>\n        y = array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=array([1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\n/home/francesca/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=2047869876, splitter='best'), X=<2x2 sparse matrix of type '<class 'numpy.float3...ored elements in Compressed Sparse Column format>, y=array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]]), sample_weight=array([1., 1.]), check_input=False, X_idx_sorted=None)\n    231 \n    232         self.max_features_ = max_features\n    233 \n    234         if len(y) != n_samples:\n    235             raise ValueError(\"Number of labels=%d does not match \"\n--> 236                              \"number of samples=%d\" % (len(y), n_samples))\n        y = array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [4.],\n       [3.],\n       [4.]])\n        n_samples = 2\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n\nValueError: Number of labels=8077 does not match number of samples=2\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', RandomForestClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search\n",
    "--------------------------\n",
    "\n",
    "Scikit-learn has `GridSearchCV` and `RandomizedSearchCV`. Both have the same functionality and can be used to find good parameters for the models. What is great about both these classes that they are both transformers - they return an estimator so you can chain them and put in your pipeline.\n",
    "\n",
    "**GridSearchCV** - you specify the exact values of the parameters you want to test\n",
    "**RandomizedSearchCV** - you specify ranges of parameters\n",
    "\n",
    "Exercise\n",
    "----------------------\n",
    "\n",
    "1. Use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for the models. Check at least 2 parameters.\n",
    "\n",
    "2. Inspect the attribute `cv_results_` after fitting. It gives a nice representation of the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search\n",
      "\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5868242630434334 {'vect__analyzer': 'word', 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
      "0.5836129964945845 {'vect__analyzer': 'word', 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
      "0.5825477847117491 {'vect__analyzer': 'word', 'vect__binary': True, 'vect__ngram_range': (1, 4)}\n",
      "0.5818693215380052 {'vect__analyzer': 'word', 'vect__binary': False, 'vect__ngram_range': (1, 4)}\n",
      "0.5812727409595936 {'vect__analyzer': 'word', 'vect__binary': False, 'vect__ngram_range': (1, 3)}\n",
      "0.5797024615638128 {'vect__analyzer': 'word', 'vect__binary': True, 'vect__ngram_range': (1, 3)}\n",
      "0.5796256954743786 {'vect__analyzer': 'word', 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
      "0.5763148527389625 {'vect__analyzer': 'char', 'vect__binary': True, 'vect__ngram_range': (1, 4)}\n",
      "0.5761793577179235 {'vect__analyzer': 'word', 'vect__binary': False, 'vect__ngram_range': (1, 2)}\n",
      "0.573710898932378 {'vect__analyzer': 'char', 'vect__binary': False, 'vect__ngram_range': (1, 4)}\n",
      "0.5717975553707141 {'vect__analyzer': 'char', 'vect__binary': False, 'vect__ngram_range': (1, 2)}\n",
      "0.5710912861740337 {'vect__analyzer': 'char', 'vect__binary': False, 'vect__ngram_range': (1, 3)}\n",
      "0.5696448386349116 {'vect__analyzer': 'char', 'vect__binary': True, 'vect__ngram_range': (1, 3)}\n",
      "0.5629549090737607 {'vect__analyzer': 'char', 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
      "0.5629532932574791 {'vect__analyzer': 'char', 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
      "0.5467813543904322 {'vect__analyzer': 'char', 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "             \n",
    "grid_clf = GridSearchCV(clf, params, scoring='f1_weighted', n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "#    \n",
    "#print(\"Randomized search\")\n",
    "#print()\n",
    "#    \n",
    "#params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "#          'vect__analyzer': [\"word\",\"char\"],\n",
    "#          'model__lr__dimensions': [100, 200]}\n",
    "#\n",
    "#grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "#grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "#\n",
    "#best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "#\n",
    "#for params, score, _ in best_params:\n",
    "#    print(score, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract further information from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text review contains a number of information that can be used for classification.\n",
    "Examples:\n",
    "- length of text\n",
    "- punctuation\n",
    "- emoji usage \n",
    "\n",
    "Example:\n",
    "\"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "1. Create a custom Transformer that extract the length of the review \n",
    "2. Create a custom Transformer that detect number of positive emojis \n",
    "3. Add the transformers above to the features extraction part in the pipeline above. **ATTENTION**: Make sure the transform methods return a sparse matrix (use `from scipy import sparse` `sparse.csr_matrix`), otherwise you will get dimensions error from numpy concatenation method used in FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        l = np.array([len(x) for x in X])\n",
    "        return sparse.csr_matrix(l).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CountEmojis(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def count_emoji(self, text):\n",
    "        positive = len(re.findall('(:\\)|:D|;\\)|:\\])', text))\n",
    "        return positive\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        l_pos = np.array([count_emoji(x) for x in X])\n",
    "        return sparse.csr_matrix(l_pos).T\n",
    "        #return l_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work!!!! (problem with numpy dimension sparse matrix)\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CountPunctuation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "       \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        l = np.array([len(\"\".join(_ for _ in s if _ in string.punctuation)) for s in X_tr])\n",
    "        return sparse.csr_matrix(l).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.17      0.11      0.14       202\n",
      "          2       0.15      0.08      0.10       225\n",
      "          3       0.35      0.28      0.31       696\n",
      "          4       0.35      0.27      0.31      1862\n",
      "          5       0.76      0.84      0.80      6249\n",
      "\n",
      "avg / total       0.62      0.65      0.63      9234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    0.7s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "        ('len', LengthExtractor())\n",
    "    ])),\n",
    "    ('scaling', StandardScaler(with_mean=False)),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.19      0.11      0.14       202\n",
      "          2       0.20      0.11      0.14       225\n",
      "          3       0.35      0.29      0.32       696\n",
      "          4       0.34      0.24      0.28      1862\n",
      "          5       0.75      0.85      0.80      6249\n",
      "\n",
      "avg / total       0.61      0.65      0.63      9234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    0.7s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "        ('emoji', CountEmojis())\n",
    "    ])),\n",
    "    ('scaling', StandardScaler(with_mean=False)),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
